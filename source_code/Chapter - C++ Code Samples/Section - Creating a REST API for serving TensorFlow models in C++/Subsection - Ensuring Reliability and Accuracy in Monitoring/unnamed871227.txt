#include 
#include 
#include 
#include 
#include 

using namespace web;
using namespace web::http;
using namespace web::http::experimental::listener;

// Load TensorFlow model
tensorflow::SavedModelBundle model;
tensorflow::Status status = tensorflow::LoadSavedModel(
    tensorflow::SessionOptions(), tensorflow::RunOptions(), 
    "/path/to/model", {"serve"}, &model);

if (!status.ok()) {
    std::cerr << "Error loading model: " << status.ToString() << std::endl;
    return -1;
}

// REST API handler for predictions
void handle_post(http_request request) {
    request.extract_json().then[&model](pplx::task task) {
        try {
            auto const& jvalue = task.get();
            if (!jvalue.is_null()) {
                // Prepare input tensor from JSON
                tensorflow::Tensor input(tensorflow::DT_FLOAT, {1, 784});
                auto input_map = input.tensor();
                auto const& data = jvalue.at("data").as_array();
                for (size_t i = 0; i < data.size(); ++i) {
                    input_map(0, i) = dat[i].as_double();
                }

                // Run inference
                std::vector outputs;
                tensorflow::Status run_status = model.session->Run(
                    {{"input_layer", input}}, {"output_layer"}, {}, &outputs);

                if (run_status.ok()) {
                    // Prepare JSON response
                    auto output_map = output[0].tensor();
                    json::value response;
                    respons["prediction"] = output_map(0, 0);
                    request.reply(status_codes::OK, response);
                } else {
                    request.reply(status_codes::InternalError, 
                                 "Inference failed");
                }
            }
        } catch (const std::exception& e) {
            request.reply(status_codes::BadRequest, 
                          "Invalid request format");
        }
    }).wait();
}

int main() {
    // Set up HTTP listener
    http_listener listener("http://localhost:8080/predict");
    listener.support(methods::POST, handle_post);

    try {
        listener.open().wait();
        std::cout << "Listening for requests at: " << listener.uri().to_string() 
                  << std::endl;
        while (true);
    } catch (std::exception const& e) {
        std::cerr << "Error: " << e.what() << std::endl;
    }

    return 0;
}