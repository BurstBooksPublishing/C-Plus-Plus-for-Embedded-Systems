#include 
#include 
#include 
#include 

int main() {
    // Load the TensorFlow Lite model from a file
    std::unique_ptr model =
        tflite::FlatBufferModel::BuildFromFile("model.tflite");
    if (!model) {
        std::cerr << "Failed to load model\n";
        return -1;
    }

    // Build the interpreter
    tflite::ops::builtin::BuiltinOpResolver resolver;
    std::unique_ptr interpreter;
    tflite::InterpreterBuilder(*model, resolver)(&interpreter);
    if (!interpreter) {
        std::cerr << "Failed to construct interpreter\n";
        return -1;
    }

    // Allocate tensor buffers
    if (interpreter->AllocateTensors() != kTfLiteOk) {
        std::cerr << "Failed to allocate tensors\n";
        return -1;
    }

    // Get input tensor and set input data
    float* input = interpreter->typed_input_tensor(0);
    *input = 1.0f;  // Example input data

    // Run inference
    if (interpreter->Invoke() != kTfLiteOk) {
        std::cerr << "Failed to invoke interpreter\n";
        return -1;
    }

    // Get output tensor and retrieve results
    float* output = interpreter->typed_output_tensor(0);
    std::cout << "Output: " << *output << std::endl;

    return 0;
}